{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7edcce",
   "metadata": {},
   "source": [
    "**Phase 5: Fine-Tuning Using LoRA or QLoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8e6187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ad0a834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01965865f5a74dd0bbbb54b639ecdbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "# Login to Hugging Face Hub\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c986fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f537d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Toggle LoRA vs QLoRA\n",
    "use_qlora = True  # set False to use standard LoRA (full precision)\n",
    "\n",
    "if use_qlora:\n",
    "    # QLoRA: 4-bit quantization via Transformers' BitsAndBytesConfig\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "else:\n",
    "    # Standard LoRA: full-precision adapter\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c7064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust based on model internals\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962006b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_7628\\3420278840.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_7628\\3420278840.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1731\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"database\",\n",
    "    embedding_function=embed_model,\n",
    "    persist_directory=\"database\"\n",
    ")\n",
    "texts = vectorstore.get().get(\"documents\", [])  # list[str]\n",
    "print(f\"Number of documents: {len(texts)}\")\n",
    "hf_dataset = Dataset.from_dict({\"text\": texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6364e3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee853e9a9bfb42c0af2f38b8f38e7238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "# Tokenize & batch\n",
    "tokenized_ds = hf_dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc97cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phase5_lora_llama3.2-1b\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3,                   \n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0151268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrishshandilya18\u001b[0m (\u001b[33mkrishshandilya18-pes-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\PES\\CSSEM-6\\GenAI\\Project\\ChessBot\\wandb\\run-20250422_171349-caw5zwhl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/krishshandilya18-pes-university/huggingface/runs/caw5zwhl' target=\"_blank\">./phase5_lora_llama3.2-1b</a></strong> to <a href='https://wandb.ai/krishshandilya18-pes-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/krishshandilya18-pes-university/huggingface' target=\"_blank\">https://wandb.ai/krishshandilya18-pes-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/krishshandilya18-pes-university/huggingface/runs/caw5zwhl' target=\"_blank\">https://wandb.ai/krishshandilya18-pes-university/huggingface/runs/caw5zwhl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/162 27:20, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.801100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.582500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.496900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.341000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error (Request ID: Root=1-6807876d-4a0987c475235870241f7000;ccea70cc-9b1a-4d6e-8063-98f0431db587)\n",
      "\n",
      "403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..\n",
      "Cannot access content at: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\n",
      "Make sure your token has the correct permissions. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=162, training_loss=2.510652580378968, metrics={'train_runtime': 1655.4421, 'train_samples_per_second': 3.137, 'train_steps_per_second': 0.098, 'total_flos': 1.533761789362176e+16, 'train_loss': 2.510652580378968, 'epoch': 2.9607390300230945})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_ds,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d316eb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./phase5_lora_llama3.2-1b\\\\tokenizer_config.json',\n",
       " './phase5_lora_llama3.2-1b\\\\special_tokens_map.json',\n",
       " './phase5_lora_llama3.2-1b\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./phase5_lora_llama3.2-1b\")\n",
    "tokenizer.save_pretrained(\"./phase5_lora_llama3.2-1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "898b2361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b511cb3296494bc6a135347ce46d0d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba19e7167cbd478ea99c5a8632547e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\krish\\.cache\\huggingface\\hub\\models--PES1UG22CS292--CHESSBOT_GENAI_llama3.2-1b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574acdbd0bee4f0086d6ad21ce624522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to: https://huggingface.co/PES1UG22CS292/CHESSBOT_GENAI_llama3.2-1b\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_name = \"CHESSBOT_GENAI_llama3.2-1b\"\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "api = HfApi()\n",
    "print(f\"Model uploaded to: https://huggingface.co/{api.whoami()['name']}/{repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
