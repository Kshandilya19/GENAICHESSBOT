{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6429a5b",
   "metadata": {},
   "source": [
    "**Phase 1: Implement Basic NLP Tasks (POS, Embedding Techniques)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2bec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def set_embedding_model():\n",
    "    return HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5aee79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49dc2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\PES\\CSSEM-6\\GenAI\\ChessBot\\data\"\n",
    "database_path = \"database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2246e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_database():\n",
    "    database = Chroma(\n",
    "        database_path,\n",
    "        embedding_function=set_embedding_model(),\n",
    "        persist_directory=database_path,\n",
    "    )\n",
    "    return database\n",
    "\n",
    "def delete_database():\n",
    "    if os.path.exists(database_path):\n",
    "        shutil.rmtree(database_path)\n",
    "        print(\"Database deleted\")\n",
    "    else:\n",
    "        print(\"Database does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed080991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1263 documents.\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(data_path)\n",
    "    return document_loader.load()\n",
    "\n",
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e4c53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 1731 chunks.\n"
     ]
    }
   ],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "chunks = split_text(documents)\n",
    "print(f\"Split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf628e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks_with_ids = calculate_chunk_ids(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ed35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_9208\\1165023105.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Existing documents in database: 0\n",
      "Adding new documents: 1731\n",
      "New documents added successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_database(chunks: list[Document]):\n",
    "    database = set_database()\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    existing_items = database.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Existing documents in database: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        database.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        print(\"New documents added successfully.\")\n",
    "    else:\n",
    "        print(\"No new documents to add.\")\n",
    "\n",
    "create_database(chunks_with_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
