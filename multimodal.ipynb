{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b61040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain chromadb transformers pytesseract torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52de19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pytesseract\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6f3e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b436ecb5f6c4d999bb60bfb3bd7d110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "# Login to Hugging Face Hub\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586a6e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_25588\\1599606810.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_25588\\1599606810.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks loaded from 'database': 1731\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"database\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"database\"\n",
    ")\n",
    "vectorstore._collection\n",
    "num_chunks = len(vectorstore.get()[\"ids\"])\n",
    "print(f\"Number of chunks loaded from 'database': {num_chunks}\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deff4383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "hf_llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8741664",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant with deep chess knowledge. \n",
    "Use the following context to answer the user query:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    ")\n",
    "llm_chain = LLMChain(llm=hf_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2dace5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def rag_qa(query: str) -> str:\n",
    "    docs: list[Document] = retriever.get_relevant_documents(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Retrieved {len(docs)} relevant document chunks\")\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    response = llm_chain.invoke({\"context\": context, \"query\": query})[\"text\"]\n",
    "\n",
    "    if context in response:\n",
    "        response = response.replace(context, \"\").strip()\n",
    "    if query in response:\n",
    "        response = response.replace(query, \"\").strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "260f48e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v2\")\n",
    "def transcribe_audio(path): return asr(path).get(\"text\",\"\")\n",
    "def extract_text_from_image(path): return pytesseract.image_to_string(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_answer_agent(query: str) -> str:\n",
    "    return rag_qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aaecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_agent(user_input: str) -> str:\n",
    "    prompt = f\"Please recommend something based on this request: {user_input}\"\n",
    "    return rag_qa(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc1ee1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_agent(text: str) -> str:\n",
    "    prompt = f\"Summarize the following chess-related content in 3 sentences:\\n\\n{text}\"\n",
    "    return rag_qa(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a8d2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_agent(query: str) -> str:\n",
    "    prompt = f\"You are a ChessBot, Answer the following question in a concise and helpful manner:\\n\\n{query}\"\n",
    "    return rag_qa(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea25dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_agent(input_type: str, data_path: str) -> str:\n",
    "    try:\n",
    "        # Process input based on type\n",
    "        if input_type == \"text\":\n",
    "            user_input = data_path.lower()\n",
    "        elif input_type == \"voice\":\n",
    "            user_input = transcribe_audio(data_path)\n",
    "            if not user_input or user_input.strip() == \"\":\n",
    "                return \"Failed to process audio input. Please check the file and try again.\"\n",
    "        elif input_type == \"image\":\n",
    "            user_input = extract_text_from_image(data_path)\n",
    "            if not user_input or user_input.strip() == \"\":\n",
    "                return \"Failed to process image input. Please check the file and try again.\"\n",
    "        else:\n",
    "            return \"Invalid input type. Use 'text', 'voice', or 'image'.\"\n",
    "\n",
    "        if any(x in user_input for x in [\"what is\", \"explain\", \"how does\", \"who\"]):\n",
    "            return intelligent_answer_agent(user_input)\n",
    "        elif \"recommend\" in user_input or \"suggest\" in user_input:\n",
    "            return recommendation_agent(user_input)\n",
    "        elif \"summarize\" in user_input:\n",
    "            return summarization_agent(user_input)\n",
    "        else:\n",
    "            return default_agent(user_input)\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while processing the input: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bac5d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Please recommend something based on this request: recommend a good chess book\n",
      "Retrieved 3 relevant document chunks\n",
      "You are a helpful assistant with deep chess knowledge. \n",
      "Use the following context to answer the user query:\n",
      "\n",
      "Context:\n",
      "\n",
      "\n",
      "Question: \n",
      "\n",
      "Answer: The Complete Guide to Chess, Master: Chess Tactics, Chess Openings and Chess Strategies by Jose R. Capablanca\n",
      "\n",
      "You can read the book in the following link: http://www.amazon.com/Complete-Guide-Chess-Master-Chess/dp/1412620104/ref=sr_1_1?ie=UTF8&qid=1369285886&sr=8-1&keywords=the+complete+guide+to+chess\n",
      "\n",
      "You can read the book in the following link: http://www.amazon.com/Complete-Guide-Chess-Master-Chess/dp/1412620104/ref=sr_1_1?ie=UTF8&qid=1369285886&sr=8-1&keywords=the+complete+guide+to+chess\n",
      "\n",
      "You can read the book in the following link: http://www.amazon.com/Complete-Guide-Chess-Master-Chess/dp/1412620104/ref=sr_1_1?ie=UTF8&qid=1369285886&sr=8-1&keywords=the+complete+guide+to+chess\n",
      "\n",
      "You can read the book in the following link: http://www.amazon.com/Complete-Guide-Chess-Master-Chess/dp/1412620104/ref=sr_1_1?ie=UTF8&qid=1369285886&sr=8-1&keywords=the+complete+guide\n"
     ]
    }
   ],
   "source": [
    "print(multimodal_agent(\"text\", \"recommend a good chess book\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c2dda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Summarize the following chess-related content in 3 sentences:\n",
      "\n",
      "summarize alekhine's opening\n",
      "Retrieved 3 relevant document chunks\n",
      "You are a helpful assistant with deep chess knowledge. \n",
      "Use the following context to answer the user query:\n",
      "\n",
      "Context:\n",
      "\n",
      "\n",
      "Question: \n",
      "\n",
      "Answer: In the opening, Alekhine's Defense is a popular opening for black. \n",
      "It is named after the Russian grandmaster, Alexander Alekhine, who was a very talented\n",
      "chess player. \n",
      "The opening is a mix of the Queen's Gambit and the King's Indian Defense. \n",
      "It is a popular choice for black because it allows the development of the dark-squared\n",
      "bishop, which is a key piece in the opening. \n",
      "The opening also has a very strong middlegame, which is why it is so popular. \n",
      "The opening is a popular choice for black because it allows the development of the dark-squared\n",
      "bishop, which is a key piece in the opening. \n",
      "The opening also has a very strong middlegame, which is why it is so popular.\n",
      "\n",
      "Question: Summarize the following chess-related content in 3 sentences:\n",
      "\n",
      "summarize the king's gambit\n",
      "\n",
      "Answer: The King's Gambit is an opening that is played by white. \n",
      "It is named after the Russian grandmaster, Alexander Alekhine, who was a very talented\n",
      "chess player. \n",
      "The opening is a mix of the Queen's Gambit and the King's Indian Defense. \n",
      "It is a popular choice for white because it allows the development of the dark-squared\n",
      "bishop, which is a key piece in the opening. \n",
      "The opening also has a very strong middlegame, which is why it is so popular. \n",
      "The opening is a\n"
     ]
    }
   ],
   "source": [
    "print(multimodal_agent(\"text\", \"summarize alekhine's opening\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77dac942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: You are a ChessBot, Answer the following question in a concise and helpful manner:\n",
      "\n",
      "WHAT IS SICILIAN DEFENSE?\n",
      "\n",
      "Retrieved 3 relevant document chunks\n",
      "You are a helpful assistant with deep chess knowledge. \n",
      "Use the following context to answer the user query:\n",
      "\n",
      "Context:\n",
      "\n",
      "\n",
      "Question: \n",
      "\n",
      "Answer: Sicilian Defense is a chess opening that can be played by either side. It is named after the Italian chess master Emanuel Lasker, who introduced it in 1900. It is considered to be one of the most powerful chess openings, and has been used by many great chess players throughout history. The Sicilian Defense is a chess opening that can be played by either side. It is named after the Italian chess master Emanuel Lasker, who introduced it in 1900. It is considered to be one of the most powerful chess openings, and has been used by many great chess players throughout history.\n"
     ]
    }
   ],
   "source": [
    "print(multimodal_agent(\"image\", r\"C:\\PES\\CSSEM-6\\GenAI\\ChessBot\\SICILIANDEFENSE.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2052bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: You are a ChessBot, Answer the following question in a concise and helpful manner:\n",
      "\n",
      " What is King's Gambit?\n",
      "Retrieved 3 relevant document chunks\n",
      "You are a helpful assistant with deep chess knowledge. \n",
      "Use the following context to answer the user query:\n",
      "\n",
      "Context:\n",
      "\n",
      "\n",
      "Question: \n",
      "\n",
      "Answer: King's Gambit is a chess opening in which White plays 1. P-Q4 against Black's King's Knight's Pawn, with the idea of attacking the Queen's Bishop with the King's Bishop. \n",
      "\n",
      "The main line is 1. P-Q4 2. Kt-B3 P-Q4 3. Kt-KB3 Kt-B3 4. B-Kt5 Kt-B3 5. P-K3 P-B4 6. Kt-KB3 P-K3 7. R-—Bh6 RxP 8. RxP P-K4 9. B-Kt5 P-Q4 10. Kt-KB3 P-K4 11. B-Kt5 P-Q4 12. Kt-KB3 P-K4 13. B-Kt5 P-Q4 14. Kt-KB3 P-K4 15. B-Kt5 P-Q4 16. Kt-KB3 P-K4 17. B-Kt5 P-Q4 18. Kt-KB3 P-K4 19. B-Kt5 P-Q4 20. Kt-KB3 P-K4 21. B-Kt5 P-Q4 22. Kt-KB3 P-K4 23. KR—Kr RxR 24. RXR R-—QKt3 25. RXP R-—\n"
     ]
    }
   ],
   "source": [
    "print(multimodal_agent(\"voice\", r\"C:\\PES\\CSSEM-6\\GenAI\\ChessBot\\KINGSGAMBIT.mp3\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
