{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc90537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca7d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./phase5_lora_llama3.2-1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed1692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 3️⃣ Load base LLaMA‑3.2‑1B model (frozen weights)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b778e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(base_model, model_path, torch_dtype=torch.float16)\n",
    "\n",
    "# 5️⃣ Build a text‑generation pipeline\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36699053",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are a grandmaster-level chess commentator providing live analysis. \"\n",
    "    \"Use the context below to craft your answer in a clear, engaging style.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"- Castling is the only move where two pieces (king and rook) move simultaneously.\\n\"\n",
    "    \"- It improves king safety, connects rooks, and can be done kingside or queenside.\\n\"\n",
    "    \"- There are specific conditions: neither piece has moved before, no pieces between them, and the king is not in check or moves through/in to check.\\n\\n\"\n",
    "    \"Question:\\n\"\n",
    "    \"Describe when and why you would choose to castle in a high‑level game. \"\n",
    "    \"Include notation examples, typical move orders leading to castling, strategic pros and cons of kingside vs queenside castling, \"\n",
    "    \"and common pitfalls to avoid.\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cae419ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a grandmaster-level chess commentator providing live analysis. Use the context below to craft your answer in a clear, engaging style.\n",
      "\n",
      "Context:\n",
      "- Castling is the only move where two pieces (king and rook) move simultaneously.\n",
      "- It improves king safety, connects rooks, and can be done kingside or queenside.\n",
      "- There are specific conditions: neither piece has moved before, no pieces between them, and the king is not in check or moves through/in to check.\n",
      "\n",
      "Question:\n",
      "Describe when and why you would choose to castle in a high‑level game. Include notation examples, typical move orders leading to castling, strategic pros and cons of kingside vs queenside castling, and common pitfalls to avoid.\n",
      "\n",
      "Answer: When there is a clear advantage to castling, and the opponent's king is in check, and the opponent is unable to prevent the king from being castled, then castling is often the best way to proceed. The main advantage of castling is to improve king safety, connect rooks, and can be done kingside or queenside. The main disadvantage of castling is that it is a high-risk move, and it is often better to avoid castling unless there is a clear advantage.\n",
      "\n",
      "The main conditions for castling are:\n",
      "\n",
      "1. Neither piece has moved before\n",
      "2. No pieces between them\n",
      "3. The king is not in check or moves through/in to check\n",
      "\n",
      "A good example of when to castling is in the game of Ruy Lopez (also known as the French Defence). In the game, Black has the advantage of having a central pawn on c3 and the opponent's king is on f1. The game continues:\n",
      "\n",
      "1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. d4 d6 6. O-O Nxd4 7. Nxd4 Bg4 8. Bxc6 Bxc6 9. Nc3 O-O 10. Nxe5 dxe5 11. Bb3 c6 12. Re1 Bf5 13. h3 Bh6 14. Rd1 Bg4 15. Nf3 Bxf3 16. gxf3 Bxf2+ 17. Kxf2 Qe7 18. Kg1 Re8 19. Rxe8+ Qxe8 20. Qh5+ Kf8 21. Qg4+ Ke7 22. Qh5+ Kd7 23. Qg4+ Ke6 24. Qh5+ Kd7 25. Qh4+ Ke6 26. Qh3+ Kd7 27. Qh5+ Ke6 28. Qh6+ Kf7 29. Qh5+ Ke6 30. Qh6+ Kf7 31. Qh5+ Ke6 32. Qh6+ Kf7 33. Qh5+ Ke6 34. Qh6+ Kf7 35. Qh5+ Ke6 36. Qh6+ Kf7 37. Qh5+ Ke6 38. Qh6+ Kf7 39. Qh5+ Ke6 40. Qh6+ Kf7 41. Qh5+ Ke6 42. Qh6+ Kf7 43. Qh5+ Ke6 44. Qh6+ Kf7 45. Qh5+ Ke6 46. Qh6+ Kf7 47. Qh5+ Ke6 48. Qh6+ Kf7 49. Qh5+ Ke6 50. Qh6+ Kf7 51. Qh5+ Ke6 52. Qh6+ Kf7 53. Qh5+ Ke6 54. Qh6+ Kf7 55. Qh5+ Ke6 56. Qh6+ Kf7 57. Qh5+ Ke6 58. Qh6+ Kf7 59. Qh5+ Ke6 60. Qh6+ Kf7 61. Qh5+ Ke6 62. Qh6+ Kf7 63. Qh5+ Ke6 64. Qh6+ Kf7 65. Qh5+ Ke6 66. Qh6+ Kf7 67. Qh5+ Ke6 68. Qh6+ Kf7 69. Qh5+ Ke6 70. Qh6+ Kf7 71. Qh5+ Ke6 72. Qh6+ Kf7 73. Qh5+ Ke6 74. Qh6+ Kf7 75. Qh5+ Ke6 76. Qh6+ Kf7 77. Qh5+ Ke6 78. Qh6+ Kf7 79. Qh5+ Ke6 80. Qh6+ Kf7 81. Qh5+ Ke6 82. Qh6+ Kf7 83. Qh5+ Ke6 84. Qh6+ Kf7 85. Qh5+ Ke6 86. Qh6+ Kf7 87. Qh5+ Ke6 88. Qh6+ Kf7 89. Qh5+ Ke6 90. Qh6+ Kf\n"
     ]
    }
   ],
   "source": [
    "outputs = gen(prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
